{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0d3168-e103-496d-ace0-2e125a48c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7529af0-a953-4aa2-98a0-1602694bca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "'''\n",
    "Block Size is the context size of our model. Context length determines the window of input that the transformer considers to generate the next meaningful output. If set to small, \n",
    "it would not take into consideraton the dependable words that affect the output, and if set to big, it owuld take in words that have no contribution to \n",
    "meaning of the output. Actual GPTs have context limits of 3000 to 7000 words.\n",
    "The batch size signifies how many batches of input and output pairs will we use in parallel. We need to work parallel since GPTs are trained on huge amounts of data. \n",
    "'''\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2881833-cb33-4802-bf91-aa553e7dfacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Reading the Shakespear poems text file. You can find the text file using the following link.\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Lets find all the unique characters that occur in this text file\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars) # Total size of all the unique character present in our text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4bf4dbb-c6aa-43c7-8250-eb065adb4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "'''\n",
    "We enumerate the each charcter in the text file and then store its index and character itself in i and ch respectively\n",
    "Encode takes a string s and for each character c in the string it gives its index by using the dictionary stoi.\n",
    "Decode does the reverse of it by taking the list of integers and giving the respective character c. \n",
    "The characters are then joined using join function with no spaces to form a string. \n",
    "''' \n",
    "stoi = { ch:i for i,ch in enumerate(chars) } \n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a86ab83-7dc3-4d37-8c8e-cdcaba47fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "# A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.\n",
    "# We convert the whole text file into a 1D pytorch tensor of long int 64 type.\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # Encodes the whole text file into a multi-dimensional matrix\n",
    "n = int(0.9*len(data)) # first 90% will be train set, rest validation set\n",
    "train_data = data[:n] \n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ad2914-4e98-46de-ba6d-e59d0986719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    '''\n",
    "    The function generates a small batch of training or validation data depending upon the required split.\n",
    "    The function will then take a randomized starting point i, but that starting point cant be such that i + block size(which is the context length) \n",
    "    exceeds the tensor length. Thus we subtract the block size from the length of data. The randint will give a 1d array of end position and number of elements.\n",
    "    We now stack the data one by one using torch.stack to form the feature tensor, for the label tensor we just increment our starting and ending position by one.\n",
    "    The label is for a word is the next word in the sentence. \n",
    "    '''\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # Moves the batches to GPU, if they are available\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd477b39-daca-4944-b06d-c8ebc3d04059",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Prevents PyTorch from tracking computations for autograd (gradient calculations), since we're not training, just evaluating.\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # Switch to evaluation mode (important for dropout, batchnorm, etc.)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) # This gives the number of times we want to compute loss to get a good estimate. losses is a tensor that will store each loss value\n",
    "        for k in range(eval_iters): \n",
    "            X, Y = get_batch(split) # Sample one batch of input/output pairs\n",
    "            logits, loss = model(X, Y) # Run forward pass\n",
    "            losses[k] = loss.item() # Store scalar loss value\n",
    "        out[split] = losses.mean() # Average loss over eval_iters batches\n",
    "    model.train() # Switch back to training mode\n",
    "    return out # {'train': avg_train_loss, 'val': avg_val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cbf9803-4a7f-4fd3-ba57-be07a8250d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    This class defines the Bigram model(predicts the next word based on the current one, two words at play thus Bigram).\n",
    "    The class inherits all the methods and properties from nn.Module.\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    __init__ is our constructor, together with super() it calls the base class(i.e nn.module)\n",
    "    We make a embedding lookup table matrix of size vocab_size by vocab_size. The embedding matrix contains vectors for the index of each character\n",
    "    that are randomised initially. Each vector contains vocab_size number of values.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    '''\n",
    "    idx is of the for (B,T). B is the batch size and T is the context length. Targets are next token tensor that will be used for training, \n",
    "    they are optional. Each element in the idx is a index value of the character present inside the mini batch that we are using for a single iteration.\n",
    "    PyTorch replaces each index with the corresponding row (vector) from the embedding matrix, which has shape (vocab_size, vocab_size).\n",
    "    So for each token (integer) in idx, it fetches its associated vector of logits (length = vocab_size) from the embedding table.\n",
    "    As such, this idx will now become a 3D tensor as each index value of a character is now replaced by its vector of probability values inside the array\n",
    "    of batch size and context length. The shape of logits is (B,T,C) where C is the vocab_size. \n",
    "    \n",
    "    To predict the loss we use Cross-entropy. To use cross entropy we need to collapse the column dimension and produce a 2D array for logits and 1D for targets.\n",
    "    Now we have a tensor with 256 rows and 65 columns. Each row corresponds to a single token prediction, \n",
    "    and contains the 65 logits (scores) for each possible next character in the vocabulary.\n",
    "    '''\n",
    "\n",
    " \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # Sidenote: Logits are logarithm of the odds of an event, representing the raw, unnormalized output of a classification model before any activation function is applied.\n",
    "        logits = self.token_embedding_table(idx) # Shape of logits is (B,T,C), C being the vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context. There is no voacb_size present here.\n",
    "        '''\n",
    "        The logits and the loss are calculated for each iteration using the forward self function. The last logit is used to calculate the next character\n",
    "        in our sequence since after passing through the transformer the whole of the sentence will have its meaning baked in the last character. \n",
    "        The targets are set to none, as such their is no reshaping of logits dimensions thus uses [:, -1, :] and outputs (B,C) since T is 1.\n",
    "        Softmax is then applied to logits to convert those arrays of probabilistic numbers into normal distributive values between 0 and 1.\n",
    "        Now we sample from this distribution to get the next token of our sequence using torch.multinomial and then concatenate it with idx to form the longer context.\n",
    "        '''\n",
    "        for _ in range(max_new_tokens):\n",
    "            # We get the logits and loss from the forward self function.\n",
    "            logits, loss = self(idx) \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2943df49-678e-4581-b6d8-78dc238da9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7305, val loss 4.7241\n",
      "step 300: train loss 2.8110, val loss 2.8249\n",
      "step 600: train loss 2.5434, val loss 2.5682\n",
      "step 900: train loss 2.4932, val loss 2.5088\n",
      "step 1200: train loss 2.4863, val loss 2.5035\n",
      "step 1500: train loss 2.4665, val loss 2.4921\n",
      "step 1800: train loss 2.4683, val loss 2.4936\n",
      "step 2100: train loss 2.4696, val loss 2.4846\n",
      "step 2400: train loss 2.4638, val loss 2.4879\n",
      "step 2700: train loss 2.4738, val loss 2.4911\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size) # Creates an instance of our Bigram model. This helps create the embedding table of vocab_size by vocab_size.\n",
    "# When we call the model next, we would need to specify our features and labels i.e x and y. \n",
    "m = model.to(device) # Switches to GPU if available \n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # Sets the optimizer parameters.\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    # Useful to check for overfitting\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train') # Training dataset used\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) # Logits and loss are calculated \n",
    "    optimizer.zero_grad(set_to_none=True) # Clears previous gradient\n",
    "    loss.backward() # Backpropagates the loss to compute gradients.\n",
    "    optimizer.step() # Updates model weights using gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81a6d281-473e-47cc-b233-3c6c38af8dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "od nos CAy go ghanoray t, co haringoudrou clethe k,LARof fr werar,\n",
      "Is fa!\n",
      "\n",
      "\n",
      "Thilemel cia h hmboomyorarifrcitheviPO, tle dst f qur'dig t cof boddo y t o ar pileas h mo wierl t,\n",
      "S:\n",
      "STENENEat I athe thounomy tinrent distesisanimald 3I: eliento ald, avaviconofrisist me Busarend un'soto vat s k,\n",
      "SBRI he the f wendleindd t acoe ts ansu, thy ppr h.QULY:\n",
      "KIIsqu pr odEd ch,\n",
      "APrnes ouse bll owhored miner t ooon'stoume bupromo! fifoveghind hiarnge s.\n",
      "MI aswimy or m, wardd tw'To tee abifewoetsphin sed The a\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # Context is the first initial input to the model. It's just a seed to start generation.\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist())) # Generates 500 tokens. Predict next token probabilities from last token. \n",
    "# Sample from those probabilities. Append the new token to the input sequence.\n",
    "# Finally it prints the shakespeare like artificial text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37184536-4ebf-441d-8e4d-db837ece3f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
